### 总体步骤
- [ ] 下载数据和代码
-  > 地址：https://drive.google.com/drive/folders/1xQBRDs5q_2xLOdOpbq7UeAmUM0Ht370A
-  > experiment_data里的mixed？？？在哪呢
- [ ] 使用transformer，在上一步下载的数据集上跑一遍，需要得到如下指标的数据：
  - [ ] PPL
  - [ ] BLEU SR
  ###### BLEU是衡量机器翻译质量的算法。
  - BLEU的输出介于0到1之间，越接近一，说明机器翻译越接近人类。
  - SR 是数据集
  - MR是multi_reference_test.json
  
  - [ ] BLEU MR:用BLEU算法评测Transformer模型，数据集为MR
  - [ ] Rouge-1 SR:系统和参考摘要之间的单字组 （每个单词）的重叠。
  - [ ] Rouge-1 MR
  - [ ] Rouge-2 SR:系统和参考摘要之间的双字节重叠。
  - [ ] Rouge-2 MR
  - [ ] Rouge-L SR:基于最长公共子序列（LCS）的统计数据
  - [ ] Rouge-L MR
##### Transformer实验
###### 环境要求：
  - Python 3.6.3 (本地机器：3.6.8）
  - Tensorflow 1.12.0 （本地机器1.14.0)
  - numpy >= 1.15.4 （本地机器1.16.4）
  - sentencepiece 0.1.8 （本地机器 0.1.83）
  - tqdm >=4.28.1 (本地机器 4.36.1）
###### 本地机器配置
  - CPU为：4个Intel(R) Core(TM) i5-4590 CPU @ 3.30GHz 
  - 内存16Gb
  - 显卡 Intel E3-1200 v3/4th,非独立显卡
###### 训练步骤
  1. **把训练数据放到iwslt2016/de-en文件夹下**
  - 实际操作步骤1：通过git clone https://github.com/Kyubyong/transformer.git 600gb/NLP0,把transformer的代码下载到了本地的600gb/NLP0位置。
  - 运行bash download.sh下载数据集到NLP0的目录下
  2. **用命令：python prepro.py预处理train/eval/test data** 
    - 如果要改变vocabulary size(默认32000），python prepro.py --vocab_size 8000
    - 这个命令将创建两个文件夹：iwslt2006/prepro和iwslt2016/segmented
  3. **运行命令python train.py**
    - 要看哪个参数是可能的，请查看hparams.py
    - 命令行举例：python train.py --logdir myLog --batch_size 256 --dropout_rate 0.5
    - 然而，09/23/21：45实验失败了。报错： not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global-jit was not set
  4. **也可以直接下载预训练模型,替代3**
    - wget -qO- --show-progress https://dl.dropbox.com/s/efv2gmq5hu3np43/log.tar.gz | tar xz
    
 0. **然后你将看到训练损失曲线loss,学习率lr,bleu score on devset**
 1. **测试接口**
    - python test.py --ckpt log/1/iwslt2016_E17L2.78-26078 (OR yourCkptFile OR yourCkptFileDirectory)
